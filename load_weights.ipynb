{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6000 TRAIN, 4000 TEST\n",
    "#no signal (0) BLUE , electron/positron (1) GREEN , another kind of particles (2) YELLOW\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import tables\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "train_file = 'dataset/train_1-2.hdf5'\n",
    "test_file = 'dataset/test_1-2.hdf5'\n",
    "#submission_file = 'submission_1-2.hdf5'\n",
    "\n",
    "f=tables.open_file(train_file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_events(start_index=0, end_index=100):\n",
    "    X_train, y_train, w_train = [], [], []\n",
    "    for event_index in trange(start_index, end_index):\n",
    "        data_hits = np.array(f.root.data[event_index : event_index + 1])\n",
    "        data_labels=np.array(f.root.label[event_index : event_index + 1],dtype=np.int64)\n",
    "        data_weights = np.zeros([1,192,192,192],dtype=np.float32)\n",
    "        vals, counts = np.unique(data_labels,return_counts=True)\n",
    "        for i, val in enumerate(vals):\n",
    "            if val==0:\n",
    "                data_weights[np.where(data_labels == val)] = 0.\n",
    "            else:\n",
    "                data_weights[np.where(data_labels == val)] = 1. / counts[i] / len(vals)\n",
    "        X_train.append(data_hits)\n",
    "        y_train.append(data_labels)\n",
    "        w_train.append(data_weights)\n",
    "    return np.concatenate(X_train), np.concatenate(y_train), np.concatenate(w_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import commands,sys,os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.platform\n",
    "import tensorflow.contrib.layers as L\n",
    "import tensorflow.contrib.slim as slim\n",
    "IMAGE_SIZE=192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_test(num_images,X,y,w):\n",
    "    images_plot, labels_plot, weights_plot, preds_plot, probs_plot = sess.run([images, labels, weights, prediction, softmax], feed_dict={images: X,\n",
    "                                                                                                                                         labels: y,\n",
    "                                                                                                                                         weights: w})\n",
    "    \n",
    "    if num_images is None or num_images > len(images_plot):\n",
    "        num_images = len(images_plot)\n",
    "    \n",
    "    for index in range(num_images):\n",
    "        print \"ENERGY DEPOSITION\"\n",
    "        fig=plt.figure()\n",
    "        crop=images_plot[index].reshape([IMAGE_SIZE,IMAGE_SIZE,IMAGE_SIZE])\n",
    "        ax = fig.add_subplot(111,projection='3d')\n",
    "        x,y,z = np.where(crop>0)\n",
    "        v=crop[crop>0]\n",
    "        cv = ax.scatter(x,y,z,c=v, marker='o', vmin=0,vmax=40)\n",
    "        plt.colorbar(cv)\n",
    "        plt.show()\n",
    "    \n",
    "        print \"LABELS\"\n",
    "        fig=plt.figure()\n",
    "        data_labels=labels_plot[index].reshape([IMAGE_SIZE,IMAGE_SIZE,IMAGE_SIZE])\n",
    "        ax = fig.add_subplot(111,projection='3d')\n",
    "        x,y,z = np.where(crop>0)\n",
    "        v=data_labels[crop>0]\n",
    "        cl = ax.scatter(x,y,z,c=v, marker='o', vmin=0,vmax=2)\n",
    "        plt.colorbar(cl)\n",
    "        plt.show()\n",
    "        print np.unique(data_labels,return_counts=True)\n",
    "        \n",
    "        print \"WEIGHTS\"\n",
    "        fig=plt.figure()\n",
    "        data_labels=weights_plot[index].reshape([IMAGE_SIZE,IMAGE_SIZE,IMAGE_SIZE])\n",
    "        ax = fig.add_subplot(111,projection='3d')\n",
    "        x,y,z = np.where(crop>0)\n",
    "        v=data_labels[crop>0]\n",
    "        cl = ax.scatter(x,y,z,c=v, marker='o', vmin=1e-4,vmax=1e-3)\n",
    "        plt.colorbar(cl)\n",
    "        plt.show()\n",
    "        print np.unique(data_labels,return_counts=True)\n",
    "        \n",
    "        print \"PREDICTION\"\n",
    "        fig=plt.figure()\n",
    "        data_labels=preds_plot[index].reshape([IMAGE_SIZE,IMAGE_SIZE,IMAGE_SIZE])\n",
    "        ax = fig.add_subplot(111,projection='3d')\n",
    "        x,y,z = np.where(crop>0)\n",
    "        v=data_labels[crop>0]\n",
    "        cl = ax.scatter(x,y,z,c=v, marker='o', vmin=0,vmax=2)\n",
    "        plt.colorbar(cl)\n",
    "        plt.show()\n",
    "        print np.unique(data_labels,return_counts=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"input\"):\n",
    "    images  = tf.placeholder(tf.float32, [None, 192, 192, 192])\n",
    "    labels  = tf.placeholder(tf.int64,   [None, 192, 192, 192])\n",
    "    weights = tf.placeholder(tf.float32, [None, 192, 192, 192])\n",
    "    images3D = tf.reshape(images, [-1, 192, 192, 192, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_resnet_module(input_tensor, num_outputs, trainable=True, kernel=(3,3,3), stride=1, scope='noscope'):\n",
    "\n",
    "    fn_conv = slim.conv3d\n",
    "\n",
    "    num_inputs  = input_tensor.get_shape()[-1].value\n",
    "    with tf.variable_scope(scope):\n",
    "        shortcut = None\n",
    "        if num_outputs == num_inputs and stride ==1 :\n",
    "            shortcut = input_tensor\n",
    "        else:\n",
    "            shortcut = slim.conv3d(inputs      = input_tensor,\n",
    "                                   num_outputs = num_outputs,\n",
    "                                   kernel_size = 1,\n",
    "                                   stride      = stride,\n",
    "                                   normalizer_fn = slim.batch_norm,\n",
    "                                   activation_fn = None,\n",
    "                                   trainable   = trainable,\n",
    "                                   scope       = 'shortcut')\n",
    "        residual = slim.conv3d(inputs      = input_tensor,\n",
    "                               num_outputs = num_outputs,\n",
    "                               kernel_size = kernel,\n",
    "                               stride      = stride,\n",
    "                               normalizer_fn = slim.batch_norm,\n",
    "                               #activation_fn = None,\n",
    "                               trainable   = trainable,\n",
    "                               scope       = 'resnet_conv1')\n",
    "        \n",
    "        residual = slim.conv3d(inputs      = residual,\n",
    "                               num_outputs = num_outputs,\n",
    "                               kernel_size = kernel,\n",
    "                               normalizer_fn = slim.batch_norm,\n",
    "                               activation_fn = None,\n",
    "                               trainable   = trainable,\n",
    "                               scope       = 'resnet_conv2')\n",
    "        \n",
    "        return tf.nn.relu(shortcut + residual)\n",
    "\n",
    "def double_toy_resnet(input_tensor, num_outputs, trainable=True, kernel=3, stride=1, scope='noscope'):\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "\n",
    "        resnet1 = toy_resnet_module(input_tensor=input_tensor,\n",
    "                                    trainable=trainable,\n",
    "                                    kernel=kernel,\n",
    "                                    stride=stride,\n",
    "                                    num_outputs=num_outputs,\n",
    "                                    scope='module1')\n",
    "        \n",
    "        resnet2 = toy_resnet_module(input_tensor=resnet1,\n",
    "                                    trainable=trainable,\n",
    "                                    kernel=kernel,\n",
    "                                    stride=1,\n",
    "                                    num_outputs=num_outputs,\n",
    "                                    scope='module2')\n",
    "\n",
    "        return resnet2\n",
    "\n",
    "def toy_uresnet(input_tensor, num_class, reuse=False, trainable=True, base_filter=16, num_contraction=4):\n",
    "\n",
    "    with tf.variable_scope('toy_uresnet', reuse=reuse):\n",
    "    \n",
    "        conv_feature_map={}\n",
    "        net = input_tensor\n",
    "        print('Input shape {:s}'.format(net.shape))\n",
    "\n",
    "        # 1st conv layer normal\n",
    "        net = slim.conv3d     (net, base_filter, 3, normalizer_fn=slim.batch_norm, trainable=trainable, scope='conv0')\n",
    "        conv_feature_map[net.get_shape()[-1].value] = net\n",
    "        print('Encoding step 0 shape {:s}'.format(net.shape))  \n",
    "\n",
    "        net = slim.max_pool3d (net,              2, scope='maxpool0')    \n",
    "        # encoding steps\n",
    "        for step in range(num_contraction):\n",
    "            num_outputs = base_filter * (2**(step+1))\n",
    "            stride = 2\n",
    "            if step == 0: stride = 1\n",
    "            net = double_toy_resnet(net, num_outputs, trainable=trainable, stride=stride, scope='res{:d}'.format(step+1))\n",
    "            conv_feature_map[net.get_shape()[-1].value] = net\n",
    "            print('Encoding step {:d} shape {:s}'.format(step+1,net.shape))\n",
    "        # decoding steps\n",
    "        for step in range(num_contraction):\n",
    "            num_outputs = net.get_shape()[-1].value / 2\n",
    "            net = slim.conv3d_transpose(net, num_outputs, 3, stride=2, normalizer_fn=slim.batch_norm, trainable=trainable, scope='deconv{:d}'.format(step))\n",
    "            net = tf.concat([net, conv_feature_map[num_outputs]], axis=len(net.shape)-1, name='concat{:d}'.format(step))\n",
    "            net = double_toy_resnet(net, num_outputs, trainable=trainable, scope='conv{:d}'.format(step+num_contraction+1))\n",
    "            print('Decoding {:d} shape {:s}'.format(step,net.shape))\n",
    "\n",
    "        # final conv layer\n",
    "        net = slim.conv3d(net, num_class, 3, normalizer_fn=slim.batch_norm, trainable=trainable, scope='lastconv')\n",
    "        print('Final shape {:s}'.format(net.shape))  \n",
    "        return net\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train net...\n",
      "Input shape (?, 192, 192, 192, 1)\n",
      "Encoding step 0 shape (?, 192, 192, 192, 8)\n",
      "Encoding step 1 shape (?, 96, 96, 96, 16)\n",
      "Encoding step 2 shape (?, 48, 48, 48, 32)\n",
      "Encoding step 3 shape (?, 24, 24, 24, 64)\n",
      "Encoding step 4 shape (?, 12, 12, 12, 128)\n",
      "Decoding 0 shape (?, 24, 24, 24, 64)\n",
      "Decoding 1 shape (?, 48, 48, 48, 32)\n",
      "Decoding 2 shape (?, 96, 96, 96, 16)\n",
      "Decoding 3 shape (?, 192, 192, 192, 8)\n",
      "Final shape (?, 192, 192, 192, 3)\n",
      "\n",
      "Building test net...\n",
      "Input shape (?, 192, 192, 192, 1)\n",
      "Encoding step 0 shape (?, 192, 192, 192, 8)\n",
      "Encoding step 1 shape (?, 96, 96, 96, 16)\n",
      "Encoding step 2 shape (?, 48, 48, 48, 32)\n",
      "Encoding step 3 shape (?, 24, 24, 24, 64)\n",
      "Encoding step 4 shape (?, 12, 12, 12, 128)\n",
      "Decoding 0 shape (?, 24, 24, 24, 64)\n",
      "Decoding 1 shape (?, 48, 48, 48, 32)\n",
      "Decoding 2 shape (?, 96, 96, 96, 16)\n",
      "Decoding 3 shape (?, 192, 192, 192, 8)\n",
      "Final shape (?, 192, 192, 192, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Building train net...')\n",
    "train_net = toy_uresnet (images3D, 3, trainable=True,  reuse=False, base_filter=8, num_contraction=4)\n",
    "\n",
    "print('\\nBuilding test net...')\n",
    "test_net  = toy_uresnet (images3D,  3, trainable=False, reuse=True, base_filter=8, num_contraction=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 192, 192, 192, 3)\n",
      "(?, 192, 192, 192)\n",
      "(?, 192, 192, 192)\n",
      "(?,)\n",
      "(?, 192, 192, 192, 3)\n",
      "(?, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable train/toy_uresnet/conv0/weights/Adam/ already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-8-692bc0f4a3c6>\", line 22, in <module>\n    train          = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e5edcf010b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mloss\u001b[0m           \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_weighted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m192\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlearning_rate\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain\u001b[0m          \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n\u001b[0;32m--> 409\u001b[0;31m                                 name=name)\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m   def compute_gradients(self, loss, var_list=None,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[1;32m    550\u001b[0m                        ([str(v) for _, _, v in converted_grads_and_vars],))\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_get_variable_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m     \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/adam.pyc\u001b[0m in \u001b[0;36m_create_slots\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Create slots for the first and second moments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"m\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.pyc\u001b[0m in \u001b[0;36m_zeros_slot\u001b[0;34m(self, var, slot_name, op_name)\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0mnamed_slots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_slots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m       \u001b[0mnew_slot_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslot_creator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_zeros_slot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m       self._restore_slot_variable(\n\u001b[1;32m    986\u001b[0m           \u001b[0mslot_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslot_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.pyc\u001b[0m in \u001b[0;36mcreate_zeros_slot\u001b[0;34m(primary, name, dtype, colocate_with_primary)\u001b[0m\n\u001b[1;32m    177\u001b[0m     return create_slot_with_initializer(\n\u001b[1;32m    178\u001b[0m         \u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslot_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         colocate_with_primary=colocate_with_primary)\n\u001b[0m\u001b[1;32m    180\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.pyc\u001b[0m in \u001b[0;36mcreate_slot_with_initializer\u001b[0;34m(primary, initializer, shape, dtype, name, colocate_with_primary)\u001b[0m\n\u001b[1;32m    151\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n\u001b[0;32m--> 153\u001b[0;31m                                 dtype)\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       return _create_slot_var(primary, initializer, \"\", validate_shape, shape,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/slot_creator.pyc\u001b[0m in \u001b[0;36m_create_slot_var\u001b[0;34m(primary, val, scope, validate_shape, shape, dtype)\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_resource_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m       validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m     66\u001b[0m   \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_partitioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_partitioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1295\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1298\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1299\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    437\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    406\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    745\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 747\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    748\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable train/toy_uresnet/conv0/weights/Adam/ already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-8-692bc0f4a3c6>\", line 22, in <module>\n    train          = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2878, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py\", line 2818, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('analysis'):\n",
    "    prediction     = tf.argmax(test_net,4)\n",
    "    accuracy_allpx = tf.reduce_mean(tf.cast(tf.equal(prediction, labels),tf.float32))\n",
    "    nonzero_idx    = tf.where(tf.reshape(images, [-1, 192, 192, 192]) > tf.to_float(0.) )\n",
    "    nonzero_label  = tf.gather_nd(labels, nonzero_idx)\n",
    "    nonzero_pred   = tf.gather_nd(tf.argmax(test_net, 4), nonzero_idx)\n",
    "    accuracy_valpx = tf.reduce_mean(tf.cast(tf.equal(nonzero_label, nonzero_pred),tf.float32))\n",
    "    softmax        = tf.nn.softmax(logits=test_net)\n",
    "    \n",
    "    nonzero_softmax = tf.gather_nd(softmax,nonzero_idx)\n",
    "    \n",
    "\n",
    "print np.shape(test_net)\n",
    "print np.shape(prediction)\n",
    "print np.shape(labels)\n",
    "print np.shape(nonzero_label)\n",
    "print np.shape(softmax)\n",
    "print np.shape(nonzero_softmax)\n",
    "with tf.variable_scope('train'):\n",
    "    loss_pixel     = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=train_net)\n",
    "    loss_weighted  = tf.multiply(loss_pixel, weights)\n",
    "    loss           = tf.reduce_mean(tf.reduce_sum(tf.reshape(loss_weighted, [-1, int(192**3)]),axis=1))\n",
    "    learning_rate  = tf.placeholder(tf.float32,[])\n",
    "    train          = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./9-05-1_zero_weights-13000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./9-05-1_zero_weights-14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./9-05-1_zero_weights-15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./9-05-1_zero_weights-16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./9-05-1_zero_weights-17000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "reader = tf.train.Saver(tf.get_collection(tf.GraphKeys.MODEL_VARIABLES))\n",
    "\n",
    "for j in range(13,18):\n",
    "    reader.restore(sess, './9-05-1_zero_weights-'+str(j*1000))\n",
    "\n",
    "    fout=open('9-05-1_zero_weights-{}.csv'.format(j*1000),'w')\n",
    "    fout.write('entry,accuracy\\n')\n",
    "    for i in range(2):\n",
    "        X_train, y_train, w_train = read_events(start_index=i, end_index=i+1)\n",
    "        #print('test accuracy on non-zero pixels%g' % sess.run(accuracy_valpx, feed_dict={images: X_train, labels: y_train, weights: w_train}))\n",
    "        acc,soft=sess.run([accuracy_valpx,nonzero_softmax], feed_dict={images: X_train, labels: y_train, weights: w_train})\n",
    "        \n",
    "        #accuracy[i]=acc\n",
    "        #fout.write('entry,accuracy\\n')\n",
    "        entry = i\n",
    "        accuracy = float(acc)\n",
    "        fout.write('%d,%g' % (entry,accuracy))\n",
    "        fout.write('\\n')\n",
    "    fout.close()\n",
    "    #imshow_test(16,X_train,y_train,w_train)\n",
    "    #print np.mean(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entry 0 accuracy 0.9208370000000001\n",
      "entry 1 accuracy 0.9270219999999999\n",
      "\n",
      "Mean accuracy 0.9239295000000001\n",
      "\n",
      "entry 0 accuracy 0.938126\n",
      "entry 1 accuracy 0.977318\n",
      "\n",
      "Mean accuracy 0.957722\n",
      "\n",
      "entry 0 accuracy 0.939945\n",
      "entry 1 accuracy 0.9497040000000001\n",
      "\n",
      "Mean accuracy 0.9448245000000001\n",
      "\n",
      "entry 0 accuracy 0.938126\n",
      "entry 1 accuracy 0.935897\n",
      "\n",
      "Mean accuracy 0.9370115\n",
      "\n",
      "entry 0 accuracy 0.9390350000000001\n",
      "entry 1 accuracy 0.897436\n",
      "\n",
      "Mean accuracy 0.9182355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "\n",
    "for j in range(13,18):\n",
    "\n",
    "    df = pd.read_csv('9-05-1_zero_weights-{}.csv'.format(j*1000),dtype={'accuracy': float})\n",
    "\n",
    "    for x in range(df.entry.size):\n",
    "        print 'entry',df.entry.values[x],'accuracy',df.accuracy.values[x]\n",
    "    print\n",
    "    print 'Mean accuracy', df.accuracy.values.mean()\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
